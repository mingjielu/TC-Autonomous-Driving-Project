WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:36,327 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:36,438 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:36,575 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:36,699 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:37,087 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:37,106 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:37,131 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:50:37,356 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

2025-04-22 06:50:37,738 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,027 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,130 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,342 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,425 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,907 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:38,936 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:39,168 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:39,168 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:39,184 - mmdet - INFO - Distributed training: False
2025-04-22 06:50:39,444 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:39,444 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:39,854 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:39,854 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:40,056 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:40,085 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:40,085 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:40,195 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,196 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,196 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,197 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,197 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,198 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,198 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,199 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,204 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,208 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,212 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,216 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,220 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,224 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,228 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,231 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:40,231 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:40,232 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,236 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,239 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,243 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,247 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,251 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,255 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,259 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,263 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,267 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,271 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,275 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,279 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,283 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,287 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,299 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,315 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,325 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:40,329 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,364 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,446 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,447 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,448 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,449 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,450 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,451 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,452 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,453 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,454 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,455 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,456 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,457 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,458 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,459 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,460 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,461 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,462 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,463 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,464 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,464 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,465 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,466 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,467 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,468 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,469 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,470 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,471 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,472 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,476 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,480 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,482 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:40,484 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,488 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,491 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,495 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,499 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,503 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,507 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,511 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,515 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,519 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,523 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,527 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,531 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,534 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,538 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,542 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,546 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,550 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,554 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,567 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,582 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,597 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,630 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:40,631 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:40,631 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,710 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,711 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,712 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,713 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,714 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,715 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,716 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,717 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,718 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,719 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,720 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,721 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,722 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,723 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,724 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,725 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,726 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,727 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,728 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,729 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,730 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,731 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,732 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,733 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,734 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,735 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,736 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,737 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:40,746 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:40,876 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,877 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,877 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,878 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,878 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,879 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,879 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,881 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,885 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,889 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,893 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,897 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,901 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,905 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,909 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,913 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,916 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,920 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,924 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,928 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,932 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,936 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,940 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,944 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,948 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,953 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,957 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,961 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,965 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,969 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,983 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:40,999 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,013 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,048 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:41,080 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:50:41,081 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:50:41,128 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,128 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,129 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,130 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,131 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,132 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,133 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,134 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,135 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,136 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,137 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,138 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,139 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,140 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,141 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,142 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,143 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,144 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,145 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,146 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,147 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,148 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,149 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,150 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,151 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,152 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,153 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,154 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,155 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,164 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:41,297 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:41,322 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:41,360 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:41,463 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,463 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,464 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,464 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,465 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,465 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,466 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,467 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,471 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,476 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,480 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,483 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,484 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,484 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,485 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,486 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,486 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,487 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,488 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,488 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,489 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,492 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,495 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,496 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,500 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,500 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,501 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,502 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,502 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,503 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,503 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,504 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,504 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,504 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,505 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,506 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,508 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,510 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,511 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,512 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,514 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,516 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,516 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,518 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,520 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,521 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,522 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,524 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,526 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,526 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,528 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,530 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,531 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,532 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,534 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,536 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,536 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,537 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:41,538 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,540 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,542 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,542 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,544 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,545 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,547 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,549 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,549 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,552 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,553 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,553 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,557 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,557 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,557 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,561 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,562 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,565 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,568 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,569 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,571 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,573 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,573 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,577 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,578 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,581 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,583 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,585 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,586 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,588 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,589 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,593 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,594 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,599 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,601 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,604 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,608 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,620 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,624 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,637 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:41,638 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,640 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,658 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,674 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:41,677 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,678 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,678 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,679 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,680 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,680 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,681 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,682 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,686 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,690 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,694 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,698 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,701 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:41,702 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,706 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,710 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,715 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,719 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,719 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,720 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,721 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,722 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,723 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,724 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,725 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,726 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,727 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,728 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,729 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,730 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,731 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,732 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,733 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,734 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,735 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,736 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,737 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,738 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,739 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,740 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,741 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,742 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,743 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,744 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,745 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,747 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,751 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,755 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): 2025-04-22 06:50:41,755 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,756 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,757 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,758 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,759 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,760 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,761 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,762 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,763 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,764 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,765 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,766 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,767 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,768 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,769 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,770 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,771 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,772 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,773 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,774 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,775 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,776 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,776 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,777 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,778 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,779 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,780 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,781 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,782 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,783 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,785 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,791 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,793 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:41,807 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,808 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,809 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,810 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,811 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,812 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,813 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,814 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,815 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,816 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:41,817 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,818 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,819 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,820 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,821 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,822 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,823 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,824 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,825 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,826 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,827 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,828 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,829 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,830 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,831 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,832 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,833 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,834 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,835 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,836 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,837 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,838 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,839 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,840 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,841 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,842 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,843 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,844 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,845 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:41,858 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:41,858 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:41,916 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:41,980 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,035 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,036 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,037 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,038 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,039 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,040 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,041 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,042 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,043 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,044 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,045 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,046 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,047 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,048 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,049 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,050 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,051 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,052 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,053 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,054 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,055 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,056 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,057 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,058 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,059 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,060 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,061 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,062 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,063 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,064 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,065 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,066 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,067 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,068 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,069 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,070 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,071 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,072 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,073 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,088 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:50:42,125 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,125 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,126 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,126 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,127 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,127 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,128 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,129 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,134 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,138 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,142 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,146 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,150 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,154 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,158 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,162 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,166 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,170 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,174 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,178 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,182 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,186 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,190 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,194 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,199 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,203 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,207 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,211 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,215 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,219 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,232 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,248 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,262 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:50:42,295 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,376 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,377 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,378 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,379 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,380 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,381 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,382 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,383 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,384 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,385 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,386 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,387 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,388 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,389 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,390 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,391 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,392 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,393 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,394 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,395 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,396 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,397 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,398 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,399 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,400 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,401 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,402 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,403 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:50:42,413 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.797 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.213 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.638 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.703 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.433 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 39.998 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 40.784 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 42.989 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.6 seconds.
======
Done reverse indexing in 6.5 seconds.
======
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.7 seconds.
======
Done reverse indexing in 6.8 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
Done reverse indexing in 6.5 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
Done reverse indexing in 6.9 seconds.
======
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
Done reverse indexing in 6.5 seconds.
======
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.469 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 34.819 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.562 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 37.219 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.940 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.848 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.8 seconds.
======
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.2 seconds.
======
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.351 seconds.
======
Reverse indexing ...
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:18,972 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:19,181 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,184 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,186 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,187 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:19,189 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:19,191 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:19,192 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:19,194 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:19,196 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:19,197 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:19,199 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:19,201 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:19,203 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:19,204 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:19,206 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:19,207 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:19,209 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:19,211 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:19,212 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:19,214 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:19,216 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:19,217 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:19,219 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:19,221 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,223 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,225 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,285 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,285 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,287 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,287 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,288 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,288 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,288 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,288 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,288 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:19,288 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:19,433 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 43.078 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.4 seconds.
======
2025-04-22 06:52:19,647 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,649 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,651 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,653 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:19,654 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:19,656 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:19,658 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:19,660 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:19,661 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:19,663 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:19,665 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:19,666 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:19,668 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:19,670 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:19,671 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:19,672 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:19,673 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:19,675 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:19,677 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:19,678 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:19,680 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:19,682 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:19,683 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:19,685 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:19,687 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,690 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,692 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,751 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,751 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,754 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,754 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,755 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,755 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,755 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,755 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,755 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:19,755 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:19,882 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,885 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,887 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,888 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:19,890 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:19,892 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:19,894 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:19,895 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:19,897 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:19,899 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:19,900 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:19,902 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:19,904 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:19,905 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:19,907 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:19,909 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:19,911 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:19,912 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:19,914 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:19,916 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:19,917 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:19,919 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:19,921 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:19,923 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:19,925 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:19,927 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:19,987 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,987 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:19,990 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,990 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:19,990 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,990 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:19,990 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,990 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:19,990 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:19,990 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
Done reverse indexing in 6.4 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:22,048 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:22,255 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:22,257 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:22,259 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:22,261 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:22,262 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:22,264 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:22,266 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:22,267 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:22,269 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:22,271 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:22,272 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:22,274 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:22,276 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:22,277 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:22,279 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:22,281 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:22,282 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:22,284 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:22,286 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:22,287 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:22,289 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:22,291 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:22,292 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:22,294 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:22,297 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:22,299 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:22,358 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:22,358 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:22,361 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:22,361 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:22,361 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:22,361 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:22,361 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:22,361 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:22,361 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:22,361 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
Done reverse indexing in 6.3 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:22,920 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:23,122 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:23,124 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:23,126 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:23,128 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:23,129 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:23,131 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:23,133 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:23,135 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:23,136 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:23,138 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:23,140 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:23,141 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:23,143 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:23,145 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:23,146 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:23,148 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:23,150 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:23,151 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:23,153 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:23,155 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:23,157 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:23,158 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:23,160 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:23,162 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:23,164 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:23,166 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:23,226 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:23,226 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:23,228 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:23,228 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:23,228 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:23,228 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:23,228 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:23,228 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:23,228 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:23,228 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
Done reverse indexing in 6.4 seconds.
======
2025-04-22 06:52:23,875 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:23,897 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:23,902 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:23,913 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:23,916 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:23,917 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:23,929 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:23,931 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:23,931 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:23,943 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:23,960 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:23,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:23,962 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:23,978 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:23,980 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:23,980 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:23,980 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:23,982 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,003 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,007 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,064 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,065 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,100 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,101 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,189 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:24,216 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:24,234 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:24,244 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,248 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,259 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,262 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,284 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:24,291 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,296 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,311 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,315 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,320 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,324 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,332 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,346 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,350 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,357 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,400 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,407 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,412 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,427 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,430 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,466 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,504 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:24,518 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:24,532 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:24,541 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,544 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,545 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:24,549 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,555 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,558 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,561 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,564 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,570 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:24,571 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,585 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:24,594 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,594 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,598 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,605 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,621 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:24,621 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,621 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,622 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,629 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:24,640 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,038 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,066 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:25,106 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:25,138 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,153 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,185 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,191 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,208 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,218 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,244 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,255 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,258 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,269 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,289 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,300 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,307 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,309 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,309 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:25,312 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:25,313 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:25,315 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:25,317 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:25,319 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:25,320 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,320 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:25,322 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,322 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:25,324 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:25,326 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:25,327 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:25,329 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:25,331 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:25,332 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:25,334 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:25,336 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:25,337 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:25,339 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:25,341 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:25,342 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:25,344 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:25,347 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:25,350 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:25,352 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:25,354 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:25,355 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,356 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:25,376 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,417 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:25,417 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:25,420 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:25,420 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:25,421 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:25,421 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:25,421 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:25,421 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:25,421 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:25,421 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:25,435 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,462 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,476 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,504 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,516 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,532 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,540 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,555 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,566 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,587 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,589 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,611 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,667 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,668 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,683 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,700 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,707 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,720 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,724 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,742 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,744 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,757 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,760 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,781 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,782 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:25,790 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,797 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:25,811 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,832 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:25,854 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:25,905 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,934 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,940 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:25,969 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:25,996 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,014 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,027 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,044 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,054 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,062 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,074 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,079 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,080 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,085 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,091 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,107 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,116 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,127 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,137 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,141 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,151 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,154 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,170 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,186 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,189 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,211 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,221 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,249 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,270 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:26,297 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:26,365 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:26,365 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:26,396 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
Done reverse indexing in 7.0 seconds.
======
2025-04-22 06:52:26,573 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,577 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,582 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:26,584 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:26,586 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:26,588 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,588 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:26,590 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:26,592 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:26,594 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:26,596 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:26,596 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,597 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:26,599 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:26,601 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:26,603 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:26,605 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:26,606 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:26,608 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:26,610 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:26,612 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:26,613 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:26,615 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:26,617 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:26,619 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:26,620 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:26,622 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:26,622 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,624 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:26,626 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:26,628 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:26,635 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,638 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:26,644 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,650 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,660 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,666 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,671 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:26,690 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:26,690 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:26,693 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:26,693 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:26,693 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:26,693 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:26,693 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:26,693 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:26,693 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:26,694 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:26,694 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:26,700 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,723 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,729 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:26,736 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,751 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,781 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:26,787 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,809 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:26,809 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:26,828 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:26,847 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:26,881 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:26,908 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,044 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,060 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,076 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,093 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,096 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,096 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,109 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,109 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,111 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,112 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,116 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,119 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,126 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,127 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,129 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,129 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,134 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,138 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,143 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,145 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,145 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,151 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,154 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,160 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,169 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,171 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,178 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,184 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,187 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,189 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,192 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,194 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,199 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,203 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,208 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,209 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,214 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,215 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,226 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,236 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,243 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,267 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,284 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,294 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,295 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,310 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,321 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,326 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,336 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,338 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,384 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,400 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,435 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,456 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,462 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,478 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,495 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,511 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,512 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,534 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,535 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,547 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,551 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,569 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,588 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,606 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,614 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,636 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,676 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,677 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,693 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,706 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,709 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,725 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,728 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,732 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,746 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,746 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,752 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:27,758 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,759 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,759 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,760 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,761 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,768 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,779 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,780 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,780 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,785 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:27,786 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,788 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,810 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,812 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:27,823 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,823 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,824 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,830 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,842 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:27,847 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,847 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,848 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,852 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,904 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:27,945 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:27,981 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,000 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,005 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,024 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,033 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,040 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,042 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,052 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,065 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,068 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,076 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,092 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,099 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,106 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,130 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,184 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,208 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,244 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,257 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,276 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,291 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,388 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,408 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,414 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,430 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,457 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,464 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,487 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,488 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,493 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,506 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,514 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,531 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,545 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,568 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,569 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,583 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,591 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,591 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,594 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,613 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,621 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,622 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,633 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,659 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,670 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,687 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,729 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,733 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,747 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,748 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,750 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,765 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,777 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,784 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,784 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:28,787 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,790 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,801 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,803 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,805 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,811 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,817 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,817 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:28,820 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,830 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,836 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,842 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,852 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,855 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,866 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,870 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,873 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,873 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,894 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:28,896 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,906 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,910 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,912 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:28,934 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,939 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:28,948 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:28,970 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,051 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,056 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,070 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,074 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,110 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,112 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,133 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,141 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,152 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,158 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,170 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,177 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,209 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,220 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,236 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,243 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,272 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,288 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,288 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:29,323 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:29,327 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,352 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,377 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:29,401 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,411 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,412 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:29,417 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,418 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:29,430 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,432 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,441 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,448 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,448 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:29,450 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,457 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,472 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,474 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,488 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,489 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,501 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 06:52:29,506 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 06:52:29,512 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,514 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,530 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,547 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,585 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,610 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,641 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,660 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,700 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,717 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,731 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,770 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,799 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,839 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,849 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,854 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:29,855 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,864 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 06:52:29,866 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,867 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,868 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 06:52:29,871 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 06:52:29,874 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 06:52:29,877 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 06:52:29,881 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 06:52:29,885 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 06:52:29,885 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,889 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 06:52:29,890 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:29,891 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,892 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 06:52:29,893 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,894 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,896 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 06:52:29,899 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 06:52:29,902 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,903 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:29,903 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 06:52:29,907 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 06:52:29,910 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 06:52:29,914 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,914 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 06:52:29,915 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,918 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 06:52:29,921 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 06:52:29,925 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,926 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 06:52:29,929 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:29,929 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 06:52:29,931 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 06:52:29,933 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 06:52:29,935 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 06:52:29,937 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 06:52:29,939 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 06:52:29,940 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,943 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 06:52:29,945 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 06:52:29,950 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,964 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:29,966 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:29,973 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:29,980 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:29,987 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,000 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,011 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,025 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:30,025 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 06:52:30,028 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:30,028 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 06:52:30,030 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:30,030 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 06:52:30,031 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:30,031 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 06:52:30,032 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:30,032 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 06:52:30,041 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,061 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,069 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,076 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,077 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,092 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,111 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,130 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,133 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,153 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,155 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,172 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,205 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,212 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,223 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,238 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,241 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,257 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,262 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,263 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,273 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,278 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,287 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,289 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,291 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,314 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,314 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,322 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,325 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,336 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,339 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,347 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,348 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,364 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,374 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:30,377 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,380 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,387 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:30,387 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:30,397 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,401 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,402 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,406 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:30,411 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,413 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,416 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,418 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,419 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:30,424 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,430 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,430 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,431 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,434 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,436 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,437 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,437 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,439 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:30,441 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,455 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,465 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,469 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,470 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,473 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,474 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,480 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,494 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,496 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,498 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,498 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,499 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,505 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,562 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,572 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:30,576 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,581 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,591 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,606 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,606 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:30,607 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,627 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,640 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,645 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,654 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,665 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,668 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,680 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,682 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,699 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,713 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,776 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,836 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,854 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,856 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,869 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,874 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:30,888 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,897 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,898 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,899 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,908 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:30,917 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,928 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,929 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,948 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,951 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:30,954 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:30,957 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,967 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:30,977 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:30,996 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,003 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,006 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,013 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,031 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,046 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,052 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,084 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,133 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,256 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,282 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,294 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,302 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,314 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,339 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,349 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,358 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,374 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,381 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,381 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,409 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,411 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,422 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,437 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,441 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,454 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,472 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,480 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,507 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,512 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,536 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,662 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,677 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,680 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,684 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,712 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,718 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,723 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,725 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,733 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,741 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,751 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,760 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,781 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,788 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,806 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,814 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,818 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,829 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,844 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,855 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,855 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,873 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,881 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,891 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,899 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,911 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,915 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,917 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:31,921 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,927 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,929 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,939 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,939 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,940 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,943 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,950 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:31,952 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,952 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,956 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,961 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,962 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,963 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,969 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,970 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,975 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:31,978 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,980 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:31,982 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,986 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,995 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:31,997 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:31,998 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:32,001 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,005 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,005 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,008 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,014 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,019 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,023 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,032 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:32,034 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,034 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,038 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,040 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,042 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,048 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,056 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,059 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:32,063 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,065 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,090 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:32,095 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,096 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,111 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,118 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,120 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,121 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,130 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,135 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,141 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,144 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,145 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,151 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,154 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,166 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,166 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,167 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,168 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,169 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,178 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,183 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,184 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,189 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,201 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,201 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,202 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,213 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,223 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,223 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,231 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,258 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,317 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,333 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,368 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,370 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,373 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,387 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,391 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,397 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,426 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,429 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,433 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,441 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,445 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,448 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,449 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,455 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,456 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,458 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,468 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,468 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,474 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,475 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,477 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,480 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,486 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,497 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,499 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,511 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,521 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,523 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,531 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,532 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,541 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,542 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,554 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,555 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,557 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,567 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,575 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,599 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,606 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,608 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,614 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,630 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,632 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,634 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,640 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,648 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,668 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,670 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,671 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,680 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,684 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,691 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:32,694 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,696 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,699 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,708 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,715 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:32,719 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,724 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:32,730 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,733 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,740 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,749 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,764 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:32,765 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:32,768 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,784 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,797 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:32,978 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,017 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,020 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,036 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,071 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,072 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,080 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,089 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:33,089 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,090 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,093 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,093 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,099 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,107 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,114 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,123 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:33,136 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,138 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,143 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,143 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,155 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,166 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,169 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,173 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,199 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,205 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,227 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,228 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,324 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,328 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,346 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,346 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,366 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,376 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,383 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:33,384 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,384 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,386 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,392 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,399 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,401 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,408 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,411 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,417 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,418 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,424 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,425 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:33,427 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,432 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,433 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,434 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,440 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,443 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,449 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,454 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,456 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,458 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,460 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,464 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,479 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,479 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,482 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,485 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,491 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,502 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,513 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,532 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,540 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:33,555 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,569 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,582 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,584 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:33,609 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,627 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,639 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,646 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:33,655 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,666 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,680 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:33,683 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,686 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,687 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,696 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:33,701 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,705 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,711 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,719 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,727 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,732 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,732 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,733 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:33,750 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,751 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,752 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,755 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,771 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,773 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,786 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,791 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,807 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,828 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,834 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,835 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,843 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,847 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,851 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,857 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,861 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,863 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,876 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,878 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,879 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,893 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,898 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,902 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,904 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,906 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,918 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,920 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,923 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,927 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,941 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,943 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:33,946 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,963 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,966 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:33,967 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:33,971 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:33,993 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,008 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,009 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,030 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,037 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,040 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,051 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,059 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,092 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,100 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,119 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,125 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,156 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,178 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,200 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,216 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,226 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,228 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,234 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,249 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,255 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,257 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,268 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,271 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,276 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,284 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:34,290 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,290 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,305 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,313 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,317 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,318 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,328 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:34,330 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,347 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,347 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,351 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,354 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,363 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,379 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,381 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,384 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,396 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,401 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,420 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,439 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,440 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,444 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,463 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,465 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,481 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,513 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,544 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,551 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,566 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,570 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,587 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,614 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,621 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,622 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,624 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,632 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,634 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,637 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,638 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:34,640 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,643 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,649 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,660 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,660 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,665 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:34,666 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,667 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,669 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,674 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,680 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,688 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,689 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,693 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,697 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,705 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,710 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,713 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,724 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,729 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,729 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,729 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,732 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,738 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,755 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,757 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,760 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,765 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,773 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,781 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:34,784 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,790 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,792 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,804 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,804 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,805 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,810 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,811 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:34,812 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:34,822 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,823 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,838 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,846 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,848 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,852 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:34,852 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,854 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,855 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,866 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,867 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,868 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,871 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,872 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,879 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,880 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,883 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,890 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,895 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,900 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,904 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,906 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,906 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,907 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,928 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,928 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,929 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,929 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,929 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,931 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:34,947 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,955 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,956 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,963 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:34,981 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,990 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:34,990 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:34,995 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,006 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,008 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,012 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,018 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,031 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,033 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,038 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,047 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,058 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,070 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,075 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,081 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,103 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,129 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:35,149 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,152 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,165 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:35,171 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,186 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,198 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,198 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,213 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,214 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,220 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,222 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,229 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,234 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:35,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,253 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,268 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,271 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,275 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:35,276 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,285 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,290 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,295 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,304 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,306 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,308 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,317 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,342 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,351 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,357 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,380 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,390 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,392 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,405 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,416 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,424 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,433 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,449 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,497 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,517 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,522 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,528 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,533 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,540 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,542 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,587 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,587 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,587 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,588 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:35,589 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,611 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,617 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,618 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,627 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:35,660 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,677 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,689 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,692 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,696 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,711 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,721 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,725 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,737 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,744 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,749 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,753 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,761 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,762 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:35,775 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,775 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,783 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,788 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,795 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:35,810 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,816 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,817 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,835 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,837 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,842 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,854 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,857 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,862 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,888 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,892 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,907 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,907 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,917 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,921 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,928 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,936 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,938 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,947 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,949 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,949 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,968 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:35,968 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:35,974 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,975 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:35,988 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:35,999 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,000 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,011 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,015 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,018 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:36,044 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,047 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:36,069 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,091 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,107 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,115 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,135 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,143 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,152 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,158 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,163 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,167 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,176 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,190 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,192 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,205 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,215 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,216 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,222 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,241 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,252 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,253 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,253 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,258 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,270 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,271 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,282 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,284 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,293 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,293 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,307 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,308 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,310 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,313 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,320 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,325 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,330 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,331 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,335 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:36,336 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,337 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,340 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,346 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,346 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,349 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,353 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,357 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,359 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,366 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,368 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:36,378 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,383 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,384 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,386 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,392 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,396 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,397 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,402 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,411 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,413 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,418 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,424 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,428 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,438 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,439 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,443 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,450 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,462 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,466 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,469 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,506 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,510 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,522 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,535 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,536 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,544 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,550 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,557 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,574 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,576 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,577 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,586 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,594 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,604 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,612 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,613 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:36,615 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,621 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,623 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,628 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,629 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,630 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,630 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,635 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,645 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,648 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,648 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:36,650 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,654 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:36,658 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,660 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,664 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,665 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,666 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,669 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,675 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,675 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,681 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,687 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,689 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,691 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,693 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:36,696 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,698 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,705 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,715 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,727 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,727 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,730 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,742 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,752 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,756 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,757 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,771 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,814 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,843 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:36,862 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,892 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:36,964 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,977 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:36,982 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,991 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,992 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:36,993 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:36,994 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,003 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,005 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,007 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,011 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,022 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:37,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,045 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,050 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,051 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,070 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,071 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,072 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,076 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,082 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,090 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,095 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,130 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,134 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,140 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,144 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,157 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,157 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,163 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,175 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,194 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,206 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,206 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:37,214 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,216 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,236 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,247 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,252 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,254 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:37,265 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,273 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,295 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,315 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,323 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,334 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,350 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,362 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,379 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,383 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,402 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,405 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,411 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,414 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,421 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,425 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,439 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,465 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,469 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,482 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,493 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,496 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,498 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,499 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,511 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,518 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,528 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,528 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,549 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,549 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,559 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,564 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,564 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,566 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,569 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,581 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,582 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,585 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,595 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,597 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,613 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:37,615 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,617 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,619 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,623 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,634 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,634 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,636 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,636 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,642 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:37,649 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,651 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,652 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,662 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,672 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,680 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,689 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,691 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,692 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,695 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,700 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,713 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,714 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,715 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,720 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,727 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,746 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,754 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,755 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,757 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,758 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,772 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,773 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,775 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,787 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,789 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,806 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,811 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,818 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,826 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,834 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,840 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,848 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,862 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,893 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,907 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,911 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,923 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,937 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,946 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,947 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:37,962 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:37,966 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:37,970 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:37,989 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:37,993 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:37,996 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,011 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,013 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,041 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,057 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,062 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,084 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,094 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,095 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,116 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,122 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,170 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,184 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,188 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:38,202 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,203 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,207 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,221 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,225 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:38,232 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,239 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,254 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,256 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,257 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,258 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,274 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,277 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,282 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,302 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,302 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,304 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,329 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,331 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,335 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,337 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,357 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,358 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,362 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,377 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,396 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,421 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,423 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,454 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,477 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,479 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,490 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,493 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,496 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,496 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,499 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,509 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,511 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,515 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,538 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,539 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,540 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,550 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,553 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,558 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,559 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,566 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,566 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,568 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,574 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,582 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,587 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,588 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,589 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,589 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,602 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,606 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,609 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,630 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,633 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,653 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,656 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,662 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,678 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,683 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,695 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,714 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,727 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,757 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,768 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,783 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,801 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:38,809 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,818 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,832 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:38,835 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,840 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,850 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,851 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,868 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,885 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,885 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,909 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,911 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,913 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,938 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:38,939 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:38,955 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:38,969 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:38,991 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:38,995 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,002 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:39,014 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,015 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,057 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,057 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,078 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,086 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,123 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,145 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,154 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,156 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,163 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,169 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,198 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,199 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,202 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,217 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,221 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,222 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,226 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,245 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,274 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,297 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,301 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,308 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,308 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,323 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,324 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,327 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,330 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,342 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,348 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,349 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,362 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,363 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,368 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,375 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,378 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,385 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,387 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,400 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,406 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,418 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,432 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,432 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,440 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,449 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,463 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,470 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,532 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,534 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,552 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,553 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,588 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,589 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,590 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,596 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,607 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,613 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,613 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,615 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,615 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,633 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,650 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,651 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,652 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,661 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,671 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,675 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,676 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,678 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,681 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,701 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,705 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,719 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,727 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,745 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,779 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,780 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,796 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,798 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,800 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,820 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,832 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,841 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,856 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,862 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,870 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,875 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,891 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,895 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,939 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:39,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:39,967 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:39,968 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:39,972 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:39,994 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:40,024 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:40,026 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,054 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:40,056 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,096 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,117 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,140 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,145 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,161 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,162 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,163 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,190 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,201 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,210 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,217 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,223 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,226 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,239 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,239 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,267 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,282 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,311 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,312 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,321 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,329 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,333 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,338 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,371 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,374 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,377 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,398 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,405 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,406 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,422 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,425 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:40,456 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:40,459 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,483 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,488 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,498 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,509 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,519 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,564 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,564 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,579 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,594 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,598 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,599 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,601 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,619 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,627 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,642 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,646 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,652 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,661 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,668 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,669 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,692 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,693 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,702 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,720 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,723 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,751 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,767 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,772 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,784 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,791 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,802 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,812 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,826 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,834 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,853 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,859 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,870 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,873 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,908 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,915 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,928 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,940 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,946 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:40,951 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,956 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,970 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:40,990 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:40,991 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:40,996 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:40,997 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,013 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,024 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:41,028 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,028 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,038 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,047 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,051 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,062 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,068 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,083 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,087 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,093 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,103 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,104 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,119 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,122 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,145 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:41,145 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,149 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,169 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,174 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,178 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:41,189 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,210 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,224 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,234 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,241 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,243 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,248 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,255 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,260 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,284 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,286 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,289 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,293 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,306 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,307 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,307 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,310 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,312 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,326 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,343 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:41,352 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,353 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,360 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,370 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,372 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:41,379 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,380 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,381 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,382 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,397 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,402 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,424 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,443 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,450 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,457 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,469 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,474 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,475 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,480 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,486 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,487 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,498 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,501 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,514 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,517 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,520 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,533 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,535 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,538 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,539 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,564 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,565 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,570 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,584 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,596 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,682 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,703 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,744 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,746 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,770 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,777 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,778 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,797 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,829 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,841 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,868 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,869 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,871 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,886 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,914 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,923 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,937 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,946 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:41,963 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:41,974 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:41,990 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:41,991 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:41,994 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:42,018 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,022 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,029 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,043 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,053 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,090 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,122 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,134 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,159 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,182 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,201 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,206 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,231 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,237 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,247 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,251 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,266 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,274 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,284 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,293 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:42,294 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,321 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,323 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:42,328 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,331 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,351 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,353 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,375 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,390 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,392 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,393 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:42,414 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,424 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,425 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:42,430 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,431 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,442 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,449 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,457 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,480 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,483 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,489 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,501 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,505 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,505 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,513 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,514 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,522 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,527 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,528 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,532 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,541 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,545 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,556 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,558 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,566 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,570 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,572 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,583 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,583 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,591 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,595 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,607 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,608 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,630 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,633 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,651 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,654 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,655 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,671 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,671 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,710 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,710 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,711 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,729 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,736 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,737 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,767 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,769 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,769 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,781 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,784 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,797 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,815 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,818 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,837 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,839 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,846 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,869 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,876 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:42,890 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,895 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,897 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,903 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:42,905 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,915 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,917 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,925 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,938 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:42,939 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,949 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,961 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,963 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:42,966 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,969 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:42,992 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:42,997 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,008 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,037 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,053 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,056 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,069 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,074 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,105 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,108 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,121 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,127 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,136 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,150 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,164 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,171 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,172 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,178 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,184 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,186 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,194 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,196 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,199 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,199 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:43,215 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,218 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,230 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:43,231 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,234 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,241 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,242 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,248 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,251 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,253 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,263 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,269 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,269 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,291 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,312 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,327 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,338 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,410 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,429 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,470 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,496 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,611 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,615 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,632 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,633 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:43,633 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,642 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,652 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,656 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,660 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,665 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:43,667 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,671 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,674 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,674 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,695 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,700 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,700 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,707 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,708 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,715 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,720 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,726 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,733 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,742 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,765 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,791 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:43,828 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:43,858 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:43,920 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,934 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,965 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:43,967 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:43,985 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:43,990 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,007 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,027 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,027 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,028 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,041 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,041 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,044 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,053 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,058 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,059 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,068 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,078 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,085 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,092 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,094 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,095 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,098 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,109 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,111 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,113 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,123 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,124 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,138 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,150 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,154 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,163 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,165 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,169 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,172 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,185 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,186 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,188 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:44,205 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,213 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,213 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,218 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:44,223 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,240 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,241 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,246 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,248 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,249 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,261 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,276 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,292 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,305 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,312 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,341 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,344 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,363 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,400 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,405 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,416 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,432 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,445 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,448 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,460 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,468 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,470 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,476 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,481 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,484 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,487 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,493 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,493 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,496 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,498 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,508 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,514 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,520 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,525 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,528 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,528 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,531 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,544 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,549 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,549 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,550 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,556 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,560 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,802 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:44,842 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:44,879 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,885 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,895 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,901 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,906 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:44,926 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:44,935 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,941 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,960 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,962 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:44,962 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:44,984 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,030 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,049 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,089 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,114 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,180 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,198 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,216 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,234 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,238 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,263 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,263 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,272 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,278 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,298 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,313 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,334 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,335 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,337 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,349 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,352 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,368 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,382 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,386 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,386 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,404 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,409 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,425 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,427 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,445 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,454 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,485 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,512 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,521 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,535 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,556 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,561 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,565 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,572 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,577 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,583 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,605 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,607 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,612 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,625 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,626 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,633 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,636 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,649 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,650 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,664 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,669 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,682 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,689 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,703 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,709 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,726 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,734 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,735 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,745 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,759 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,770 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,781 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,786 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,830 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,838 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,851 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,854 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,857 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,875 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,884 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,905 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,906 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,916 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:45,917 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:45,940 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,941 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:45,952 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:45,953 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:45,967 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:45,995 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:46,003 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,030 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:46,083 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,099 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,137 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,162 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,285 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,299 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,331 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,350 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,394 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:46,404 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,422 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,429 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:46,459 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,484 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,584 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,600 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,615 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,622 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,631 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,634 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,634 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,637 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,651 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,656 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,658 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,669 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,674 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,676 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,682 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,697 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,698 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,711 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,722 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,728 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,729 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,732 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,739 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,764 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,778 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,787 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,802 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,864 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,875 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,900 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,917 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:46,934 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,949 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,951 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:46,952 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,959 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:46,968 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,971 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:46,981 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:46,983 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:46,995 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:47,000 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,003 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,003 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,011 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:47,019 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,027 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,047 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,074 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,296 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:47,313 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:47,349 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,373 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,380 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:47,396 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:47,408 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:47,423 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:47,431 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,441 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:47,441 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:47,455 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,475 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,496 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,591 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:47,607 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:47,640 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:47,662 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:47,995 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:48,009 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:48,013 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:48,036 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:48,039 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:48,048 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:48,050 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:48,073 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:48,073 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:48,090 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:48,249 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:48,264 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:48,295 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:48,316 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:48,338 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:48,353 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:48,387 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:48,409 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:48,410 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:48,438 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:48,521 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 06:52:48,537 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 06:52:48,574 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 06:52:48,597 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 06:52:48,854 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:48,876 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,127 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,150 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,231 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,252 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,355 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,384 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,595 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,617 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,777 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,807 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,864 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,886 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:49,970 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:49,993 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:50,310 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:50,336 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:50,708 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:50,735 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:50,772 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:50,799 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:51,152 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:51,178 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:51,589 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:51,616 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:51,647 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:51,674 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:51,973 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:52,000 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:52,434 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:52,453 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:52,460 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:52,479 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:52,815 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:52,841 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 06:52:53,314 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 06:52:53,341 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 390, in _forward_single_frame_train
    det_output = self.pts_bbox_head.get_detections(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 170, in get_detections
    hs, init_reference, inter_references = self.transformer.get_states_and_refs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 218, in get_states_and_refs
    inter_states, inter_references = self.decoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py", line 97, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 841, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/decoder.py", line 295, in forward
    value = self.value_proj(value)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 40.00 MiB (GPU 0; 191.98 GiB total capacity; 17.24 GiB already allocated; 0 bytes free; 17.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 194, in forward
    query = torch.cat([value[:bs], query], -1)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 22.12 GiB already allocated; 0 bytes free; 22.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 398, in forward
    query = self.ffns[ffn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 623, in forward
    out = self.layers(x)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 21.96 GiB already allocated; 0 bytes free; 22.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 398, in forward
    query = self.ffns[ffn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 623, in forward
    out = self.layers(x)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 12.62 GiB already allocated; 0 bytes free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 398, in forward
    query = self.ffns[ffn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 623, in forward
    out = self.layers(x)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 40.00 MiB (GPU 0; 191.98 GiB total capacity; 11.60 GiB already allocated; 0 bytes free; 11.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 195, in forward
    value = self.value_proj(value)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 32.59 GiB already allocated; 0 bytes free; 33.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 194, in forward
    query = torch.cat([value[:bs], query], -1)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 32.55 GiB already allocated; 0 bytes free; 33.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 195, in forward
    value = self.value_proj(value)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 32.59 GiB already allocated; 0 bytes free; 33.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 19514 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 19509) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 766, in <module>
    main()
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 19510)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 19511)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 19512)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 19513)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 19515)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 19516)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-22_06:53:28
  host      : hjbog-srdc-20.amd.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 19509)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
