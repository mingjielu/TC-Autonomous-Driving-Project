WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:49,658 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:49,924 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:50,418 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:50,522 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:50,936 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

2025-04-22 06:58:51,250 - mmdet - INFO - Distributed training: False
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:51,384 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

2025-04-22 06:58:51,430 - mmdet - INFO - Distributed training: False
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:58:51,608 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

2025-04-22 06:58:52,217 - mmdet - INFO - Distributed training: False
2025-04-22 06:58:52,263 - mmdet - INFO - Distributed training: False
2025-04-22 06:58:52,307 - mmdet - INFO - Distributed training: False
2025-04-22 06:58:52,639 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:52,639 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:52,711 - mmdet - INFO - Distributed training: False
2025-04-22 06:58:53,054 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:53,054 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:53,405 - mmdet - INFO - Distributed training: False
2025-04-22 06:58:53,513 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:53,652 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,653 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,653 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,654 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,654 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,655 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,655 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,656 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,661 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,665 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,669 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,672 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,676 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,680 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,684 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,688 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,692 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,696 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,700 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,700 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:53,701 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:53,703 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,707 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,711 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,715 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,719 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,723 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,727 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,731 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,735 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,738 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,742 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,756 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,771 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,785 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:53,787 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:53,788 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:53,818 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,897 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,898 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,899 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,900 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,901 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,902 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,903 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,904 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,905 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,906 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,907 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,908 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,909 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,910 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,911 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,912 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,913 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,914 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,915 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,916 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,917 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,918 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,919 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,920 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,921 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,922 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,923 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,924 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:53,933 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:58:53,936 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:53,959 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:53,959 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:54,074 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,075 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,075 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,076 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,076 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,077 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,077 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,078 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,083 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,087 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,091 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,094 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,098 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,102 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,106 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,110 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,114 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,118 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,122 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,126 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,130 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,134 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,138 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,142 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,146 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,150 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,154 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,158 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,162 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,162 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:54,163 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:54,166 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,178 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,193 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,208 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,241 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:54,320 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,321 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,322 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,323 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,324 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,325 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,326 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,327 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,328 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,329 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,330 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,331 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,332 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,333 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,334 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,335 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,336 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,337 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,338 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,339 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,340 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,341 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,342 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,343 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,344 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,345 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,346 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,347 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,356 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:58:54,597 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:54,672 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:54,736 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,736 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,737 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,737 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,738 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,738 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,739 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,740 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,744 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,748 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,752 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,756 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,760 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,764 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,768 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,772 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,776 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,780 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,783 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,787 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,791 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,795 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,799 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,803 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,807 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,810 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,811 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,811 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,811 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,812 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,812 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,813 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,813 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,814 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,815 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,819 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,819 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,823 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,823 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,827 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,827 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,831 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,835 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,839 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,840 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,843 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,847 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,849 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:54,851 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,855 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,855 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,859 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,863 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,867 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,869 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,871 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,875 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,879 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,883 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,887 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,889 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:58:54,889 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 06:58:54,891 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,895 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,899 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,902 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:54,903 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,916 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,932 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,946 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,981 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:54,982 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,983 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,984 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,985 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,986 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,987 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,988 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,989 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,990 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,991 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,992 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,993 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,994 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,995 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,996 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,997 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,998 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:54,999 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,000 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,001 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,002 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,003 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,004 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,005 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,006 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,007 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,008 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,009 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,011 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,015 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,019 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,019 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:58:55,023 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,027 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,031 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,034 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,038 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,042 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,046 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,046 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:55,050 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,054 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,058 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,062 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,063 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,064 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,065 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,066 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,067 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,068 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,069 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,070 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,071 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,072 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,073 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,074 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,075 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,076 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,077 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,078 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,079 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,080 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,081 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,082 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,083 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,084 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,085 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,086 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,087 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,088 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,092 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,097 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:58:55,108 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,122 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,157 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:55,185 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,186 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,186 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,187 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,187 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,188 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,188 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,190 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,194 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,198 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,202 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,206 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,210 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,214 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,218 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,222 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,226 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,230 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,234 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,238 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,238 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,239 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,240 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,241 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,242 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,243 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,244 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,245 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,246 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,247 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,248 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,249 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,250 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,251 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,252 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,253 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,254 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,255 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,256 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,257 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,258 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,259 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,260 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,261 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,262 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,263 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,264 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,266 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,270 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,274 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,274 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 06:58:55,278 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,292 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,307 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,322 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,356 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:55,436 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,437 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,438 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,439 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,440 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,441 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,442 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,443 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,444 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,445 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,446 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,447 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,448 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,449 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,450 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,451 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,452 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,453 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,454 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,455 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,456 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,457 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,458 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,459 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,460 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,461 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,462 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,463 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:55,473 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
/opt/conda/lib/python3.9/site-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info The read operation timed out
  data = fetch_version_info()
2025-04-22 06:58:55,779 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:58:55,917 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,917 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,918 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,918 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,919 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,920 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,920 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,921 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,925 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,929 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,933 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,937 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,941 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,945 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,949 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,953 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,956 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,960 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,964 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,968 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,972 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,976 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,980 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,984 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,988 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,992 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:55,996 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,000 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,004 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,008 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,020 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,036 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,050 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:58:56,084 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,164 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,165 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,166 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,167 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,168 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,169 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,170 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,171 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,172 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,173 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,174 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,175 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,176 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,177 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,178 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,179 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,180 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,181 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,182 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,183 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,184 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,185 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,186 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,187 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,188 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,189 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,190 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,191 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,191 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,191 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,191 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:58:56,200 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
projects.mmdet3d_plugin
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 06:59:00,190 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
2025-04-22 06:59:02,050 - mmdet - INFO - Distributed training: False
2025-04-22 06:59:03,544 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 06:59:03,544 - mmdet - INFO - Set random seed to 0, deterministic: True
======
Loading NuScenes tables for version v1.0-trainval...
2025-04-22 06:59:04,440 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 06:59:04,577 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,578 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,578 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,579 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,579 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,580 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,580 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,582 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,586 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,590 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,594 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,597 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,601 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,605 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,609 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,613 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,617 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,621 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,625 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,629 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,633 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,637 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,641 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,645 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,649 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,653 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,657 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,661 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,665 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,668 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,681 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,696 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,711 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 06:59:04,745 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 06:59:04,824 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,825 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,826 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,827 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,828 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,829 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,830 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,831 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,832 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,833 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,834 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,835 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,836 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,837 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,838 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 06:59:04,839 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,840 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,841 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,842 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,843 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,844 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,845 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,846 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,847 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,848 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,849 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,850 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,851 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 06:59:04,860 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.275 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.633 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.249 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 38.340 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 37.519 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 36.904 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.2 seconds.
======
Done reverse indexing in 6.5 seconds.
======
Done reverse indexing in 7.4 seconds.
======
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 43.658 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.5 seconds.
======
Done reverse indexing in 6.5 seconds.
======
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.014 seconds.
======
Reverse indexing ...
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
Done reverse indexing in 6.5 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
Done reverse indexing in 6.7 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
Done reverse indexing in 6.2 seconds.
======
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 34.764 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.316 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.272 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.261 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.206 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.402 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.2 seconds.
======
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.4 seconds.
======
Done reverse indexing in 6.4 seconds.
======
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.147 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.4 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:31,697 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:31,912 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:31,914 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:31,916 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:31,918 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:31,920 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:31,922 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:31,923 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:31,925 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:31,927 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:31,929 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:31,930 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:31,932 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:31,934 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:31,936 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:31,938 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:31,939 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:31,941 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:31,943 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:31,945 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:31,946 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:31,948 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:31,950 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:31,951 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:31,953 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:31,956 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:31,958 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:32,018 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,018 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,020 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,020 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,021 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,021 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,021 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,021 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,021 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:32,021 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:32,469 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:32,543 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:32,676 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:32,678 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:32,680 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:32,682 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:32,684 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:32,686 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:32,687 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:32,689 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:32,691 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:32,693 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:32,694 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:32,696 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:32,698 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:32,700 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:32,701 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:32,703 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:32,705 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:32,707 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:32,708 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:32,710 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:32,712 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:32,714 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:32,715 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:32,717 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:32,720 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:32,722 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:32,741 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:32,743 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:32,745 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:32,747 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:32,748 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:32,750 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:32,752 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:32,753 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:32,755 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:32,757 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:32,758 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:32,760 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:32,762 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:32,764 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:32,765 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:32,767 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:32,769 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:32,770 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:32,772 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:32,774 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:32,775 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:32,777 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:32,778 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:32,780 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:32,782 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,782 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,784 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,784 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,785 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:32,785 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,785 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,786 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,786 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,786 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:32,786 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:32,788 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.069 seconds.
======
Reverse indexing ...
2025-04-22 07:00:32,848 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,848 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:32,850 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,850 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:32,851 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,851 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:32,851 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,851 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:32,851 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:32,851 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:33,272 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:33,467 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:33,469 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:33,471 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:33,473 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:33,474 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:33,476 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:33,478 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:33,480 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:33,481 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:33,483 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:33,485 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:33,487 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:33,488 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:33,490 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:33,492 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:33,494 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:33,495 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:33,497 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:33,499 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:33,501 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:33,502 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:33,504 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:33,506 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:33,507 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:33,510 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:33,512 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:33,572 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:33,572 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:33,575 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:33,575 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:33,576 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:33,576 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:33,576 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:33,576 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:33,577 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:33,577 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:33,905 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:34,099 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:34,102 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:34,104 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:34,106 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:34,107 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:34,109 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:34,111 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:34,113 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:34,114 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:34,116 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:34,118 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:34,120 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:34,121 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:34,123 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:34,125 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:34,127 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:34,128 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:34,130 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:34,132 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:34,134 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:34,135 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:34,137 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:34,139 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:34,141 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:34,143 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:34,145 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:34,204 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:34,204 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:34,207 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:34,207 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:34,207 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:34,207 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:34,207 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:34,207 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:34,209 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:34,209 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
Done reverse indexing in 6.5 seconds.
======
2025-04-22 07:00:36,648 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:36,662 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:36,675 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:36,682 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:36,688 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:36,698 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:36,700 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:36,700 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:36,715 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:36,715 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:36,730 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:36,746 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:36,747 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:36,748 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:36,766 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:36,767 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:36,787 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:36,793 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:36,813 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:36,818 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:36,868 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:36,873 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:36,890 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:36,898 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,160 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,186 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:37,190 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:37,196 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,223 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:37,229 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
Done reverse indexing in 6.3 seconds.
======
2025-04-22 07:00:37,233 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,234 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,244 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,249 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,249 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,275 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,280 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,281 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,281 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,294 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,295 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,296 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,301 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,302 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,310 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,327 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,341 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,349 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,361 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,394 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:37,397 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:37,399 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:37,401 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:37,402 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:37,404 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:37,406 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:37,408 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:37,410 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:37,411 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:37,413 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:37,415 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:37,417 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:37,418 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:37,420 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:37,422 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:37,424 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:37,425 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:37,427 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:37,428 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,429 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:37,430 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:37,432 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:37,434 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:37,436 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:37,438 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:37,440 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:37,447 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,457 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:37,459 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,461 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,471 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,474 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:37,475 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,477 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,486 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,486 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,501 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,502 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:37,502 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:37,503 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,505 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:37,505 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:37,505 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:37,505 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:37,505 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:37,505 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:37,506 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:37,506 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:37,507 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,508 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,517 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,524 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,529 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,529 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,534 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,540 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,555 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,664 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:37,679 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:37,704 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,709 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:37,729 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:37,736 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:37,947 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:37,975 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,050 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,068 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,106 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,193 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,200 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,203 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,210 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,216 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,220 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,221 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,239 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,242 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,244 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,251 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,252 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,264 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,272 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,272 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,274 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,276 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,281 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,281 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,285 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,287 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,300 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,301 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,302 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,305 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,332 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,335 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,336 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,336 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,338 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,356 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,357 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,357 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,358 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,358 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,411 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,445 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,470 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,472 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,504 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,506 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,526 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,577 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,597 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,607 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,626 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,631 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,648 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,657 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,676 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,683 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,686 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,700 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,706 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,710 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,713 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,716 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,736 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,739 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,741 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,742 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,743 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,746 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,746 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,750 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,753 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,754 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,762 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,767 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,767 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,768 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,769 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,776 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,793 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,802 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,803 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,804 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,804 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,805 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,820 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,832 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,863 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,906 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:38,913 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,919 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,933 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,933 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:38,937 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,966 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:38,972 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:38,975 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:38,981 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:38,982 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,001 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,013 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,017 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,030 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,036 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,038 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,050 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,056 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,059 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,083 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,097 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,108 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,125 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,125 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,135 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,151 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,181 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,210 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,274 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,310 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,315 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,331 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,366 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
Done reverse indexing in 6.6 seconds.
======
2025-04-22 07:00:39,388 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,480 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,493 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,505 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,508 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,508 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,521 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,522 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,540 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,553 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,561 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,563 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,573 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,618 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,622 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,632 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,647 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,662 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,668 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,683 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,694 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,707 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:39,716 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,717 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,731 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:39,737 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,750 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,782 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,787 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:39,817 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:39,821 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,916 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:39,952 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:39,965 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:39,969 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:40,002 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,012 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,030 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,031 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,041 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,041 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,056 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,058 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,058 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,067 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,091 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,095 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,095 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,107 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,109 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,117 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,117 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,124 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,139 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,140 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,141 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,176 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,203 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,225 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,230 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,230 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,246 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,252 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,260 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,276 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:40,280 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:40,283 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:40,286 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:40,288 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,289 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:40,292 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:40,293 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,295 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:40,298 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:40,301 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:40,304 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:40,307 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:40,310 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:40,313 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:40,316 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:40,318 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,319 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:40,321 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,322 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:40,324 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:40,327 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:40,330 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:40,333 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:40,336 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:40,339 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:40,341 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:40,344 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:40,349 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:40,352 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:40,372 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,381 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,387 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,404 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,408 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,409 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,432 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,435 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,437 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,455 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,466 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,469 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:40,469 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:40,472 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,474 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:40,474 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:40,475 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:40,475 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:40,475 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:40,475 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:40,476 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:40,476 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:40,513 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,523 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,529 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,534 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,540 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,549 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,554 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,570 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,573 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,574 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,575 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,576 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,586 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,590 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,595 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,597 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,604 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,614 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,618 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,620 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,638 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,641 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,644 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,666 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,680 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,687 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,702 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,703 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,721 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,729 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,733 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,749 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,752 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,753 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,754 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,761 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,772 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,775 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,784 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,787 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,794 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,796 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,798 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,803 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,807 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,819 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,822 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,823 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,832 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,844 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,848 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:40,856 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,858 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:40,866 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,872 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,878 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,897 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:40,897 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:40,898 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,923 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:40,947 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:40,966 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,008 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,019 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,034 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,037 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,056 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,072 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,078 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,103 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,109 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,137 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,236 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,251 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,258 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,270 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,287 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,292 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,299 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,328 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,341 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,361 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,408 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,443 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,527 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,545 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,550 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,584 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,597 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,629 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,630 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,636 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,649 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,657 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,691 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,696 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,717 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,720 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,721 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,731 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,758 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,761 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,781 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,783 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,804 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,811 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,817 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,820 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,829 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,829 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,830 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,834 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,836 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,842 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,843 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,852 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,854 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,855 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,861 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,870 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:41,871 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,874 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,876 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,894 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,895 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,909 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,925 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:41,931 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,955 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:41,956 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:41,972 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:41,974 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:41,993 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:42,004 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,005 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,040 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 07:00:42,050 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 07:00:42,078 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,080 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:42,094 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,109 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,119 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,127 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,127 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,138 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,148 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,171 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,180 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,197 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,211 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,223 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,241 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,273 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,281 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,286 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,293 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,294 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,296 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 07:00:42,299 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 07:00:42,300 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 07:00:42,302 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 07:00:42,304 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 07:00:42,306 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 07:00:42,308 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 07:00:42,309 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 07:00:42,311 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,311 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 07:00:42,313 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 07:00:42,314 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,315 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 07:00:42,316 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,317 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 07:00:42,318 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,318 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 07:00:42,320 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 07:00:42,322 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 07:00:42,324 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 07:00:42,325 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 07:00:42,326 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,326 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:42,327 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 07:00:42,329 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 07:00:42,331 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 07:00:42,332 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 07:00:42,334 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,334 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 07:00:42,336 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 07:00:42,337 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,338 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 07:00:42,338 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,341 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 07:00:42,343 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 07:00:42,361 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,363 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:42,365 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,381 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,382 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,403 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:42,403 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 07:00:42,405 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:42,405 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 07:00:42,406 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:42,406 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 07:00:42,407 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:42,407 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 07:00:42,407 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:42,407 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 07:00:42,409 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,427 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,448 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,470 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,489 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,492 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,514 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,518 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,532 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,535 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,559 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,560 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,575 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,578 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,591 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,592 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,599 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:42,600 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,618 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,620 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,622 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,625 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,633 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:42,634 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,649 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,653 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,655 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,664 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,670 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:42,691 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,696 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,704 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:42,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,724 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,729 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,771 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,799 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,838 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:42,859 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:42,907 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:42,935 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:42,988 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,014 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,026 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,043 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,053 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,053 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,062 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,066 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,066 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,076 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,081 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,084 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,086 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,087 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,090 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,102 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,109 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,120 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,128 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,129 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,129 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,137 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,150 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,153 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,156 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,157 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,158 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,166 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,173 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,178 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,183 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,207 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,229 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,232 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,262 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,274 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,276 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,295 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,295 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,298 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,339 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,339 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,353 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,365 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,365 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,367 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,368 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,387 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,391 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,401 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,406 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,408 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,450 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,455 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,480 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,484 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,492 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,495 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,508 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,516 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,527 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,540 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,542 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:43,545 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,548 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,560 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,579 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:43,620 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,634 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,667 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,670 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,694 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,696 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,745 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,747 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,767 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,768 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,776 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,791 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,822 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,843 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,848 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,854 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,873 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,879 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,881 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,889 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,926 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,951 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:43,970 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:43,971 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:43,993 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:43,998 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,003 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,024 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,030 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,040 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,044 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,048 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,063 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,074 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,082 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,090 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,108 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,114 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,116 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,119 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,131 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,166 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,177 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,188 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,200 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,224 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,233 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:44,264 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:44,270 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,294 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,296 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,312 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,315 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,330 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,339 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,351 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,373 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,376 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,392 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,402 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,406 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,421 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,422 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,436 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,441 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,453 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,462 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,475 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,483 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,513 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,515 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:44,518 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,521 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,542 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,551 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,556 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,562 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:44,570 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,584 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,584 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,611 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,612 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,619 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,641 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,641 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,649 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,650 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,656 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,690 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,691 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,714 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,716 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,717 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,721 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,727 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,731 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,732 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:44,766 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:44,791 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:44,895 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:44,933 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:44,938 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:44,956 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:44,977 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:44,978 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,025 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,033 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,047 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,050 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,051 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,055 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,061 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,062 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,074 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,076 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,082 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,092 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,097 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,110 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,115 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,120 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,123 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,129 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,141 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,145 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,146 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,151 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,154 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,156 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,174 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,181 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,185 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,195 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,199 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,223 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,246 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,250 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,255 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,270 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,274 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,283 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:45,287 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,288 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,300 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,300 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:45,301 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,306 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,312 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:45,315 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,318 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,326 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,326 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,327 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,328 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,329 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:45,343 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,344 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,348 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,351 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,354 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,366 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,370 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,371 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,398 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,399 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,416 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,421 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,421 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,432 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,434 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,442 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,447 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,454 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,456 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,465 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,472 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,479 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,484 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,485 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,488 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,493 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,495 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,502 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,504 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,508 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,520 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,526 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,527 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,543 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,544 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,548 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,574 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,628 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,660 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,685 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,700 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,717 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,721 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,730 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,746 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:45,749 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,773 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,775 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,776 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,777 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:45,789 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,812 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,820 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,821 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,852 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,874 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,883 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,896 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,901 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,901 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,918 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,935 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,940 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,956 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:45,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:45,969 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,989 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:45,990 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:45,997 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,012 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,013 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,016 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,041 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,055 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,057 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,058 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,065 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,082 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,082 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,084 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,101 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,127 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,127 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,155 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,168 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,174 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,175 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,176 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,186 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,194 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,194 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,197 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,228 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,231 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,232 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,232 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,240 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,251 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,254 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,258 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,258 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,269 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,288 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,295 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,320 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,321 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,324 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,324 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,331 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:46,332 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,336 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,341 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,355 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,363 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,368 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:46,370 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,377 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:46,378 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,392 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,395 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,401 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,406 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:46,515 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,535 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,561 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,578 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,582 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,586 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,605 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,610 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,613 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,633 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,637 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,651 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,652 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,666 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,679 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,681 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,683 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,696 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,701 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,705 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:46,717 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,723 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,724 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,733 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:46,739 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,744 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,744 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,759 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,769 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,770 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:46,782 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,786 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,788 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,795 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,797 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:46,801 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,804 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,805 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,805 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,808 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,820 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,821 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,824 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,826 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,828 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,829 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,833 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,844 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,846 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,846 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,862 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,865 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:46,868 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,870 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,873 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,882 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,883 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,883 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:46,894 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,906 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,916 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,938 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:46,988 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:46,995 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:46,998 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,010 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:47,011 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,019 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,025 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:47,032 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,038 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,039 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,039 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:47,041 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,049 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,052 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,052 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,053 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,055 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,055 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,061 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,069 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,070 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,070 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,072 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,080 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,086 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,093 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,094 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,106 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,106 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,106 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,106 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,107 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,107 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,112 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,113 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,122 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,127 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,130 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,138 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,141 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,144 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,150 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,156 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,160 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,168 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,171 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,173 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,196 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,197 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,201 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,203 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,227 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,229 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,236 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,251 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,263 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,266 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,268 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,286 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,303 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,305 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,308 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,322 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,322 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:47,331 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,334 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,337 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,340 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,347 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,350 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,354 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,356 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,357 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:47,357 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,359 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,375 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,377 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,388 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,397 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,399 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,406 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,407 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,415 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,424 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,430 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,439 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,448 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,449 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,465 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,470 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,489 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,498 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,516 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,531 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,549 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,584 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,602 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,602 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,644 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,649 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,659 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,675 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,676 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,711 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,741 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,759 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:47,769 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,782 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,788 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,794 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:47,802 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,804 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,808 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,830 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,847 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,849 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,853 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,869 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,878 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,883 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,884 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,898 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,901 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,907 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,920 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,930 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,946 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:47,954 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,966 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:47,966 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:47,973 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:47,995 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:47,998 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,007 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:48,011 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,013 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,028 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,043 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,050 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,055 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,059 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,070 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:48,089 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,107 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,107 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:48,111 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,111 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,112 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,114 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,128 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,131 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,133 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,134 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,141 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,146 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,167 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,171 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,175 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,183 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,184 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,191 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,197 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,203 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:48,204 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,205 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,208 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,209 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,215 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,222 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,224 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,235 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,237 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:48,247 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,249 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,250 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,266 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,267 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,271 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,279 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,293 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,306 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,308 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,311 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,322 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,339 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,356 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,357 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,362 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:48,375 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,377 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,379 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,391 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:48,392 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,398 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,417 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,418 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,427 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,427 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,433 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,434 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,436 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,442 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,445 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,446 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,449 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,451 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,458 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,467 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,475 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,478 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,481 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:48,484 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,485 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,486 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,488 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,493 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,496 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,508 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,512 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,514 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:48,517 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,517 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,518 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,521 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,543 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,544 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,547 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,565 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,568 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,573 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,581 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,582 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,600 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,603 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,618 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,618 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,623 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,625 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,627 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,640 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,642 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,643 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,651 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,653 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,656 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,657 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,662 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,682 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,682 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,683 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,684 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,701 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,709 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,710 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,712 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,728 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,730 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,732 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,765 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,770 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,780 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,788 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,790 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,809 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,835 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,836 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,853 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,853 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,866 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,873 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,893 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,893 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,913 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,917 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,920 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,920 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,921 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,937 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,939 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,944 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:48,952 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:48,955 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,973 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:48,981 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,983 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:48,983 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,003 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,009 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,016 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,017 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,019 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,022 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,036 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,043 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,048 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,061 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,063 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,064 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,066 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,080 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,080 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,086 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,091 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,105 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,109 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,126 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,126 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,132 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,133 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,143 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,146 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,147 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,154 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,156 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:49,162 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,164 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,167 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,171 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,189 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:49,192 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,193 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,208 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:49,212 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,215 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,224 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,228 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:49,238 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,243 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:49,244 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,246 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,268 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:49,270 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,278 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,297 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,339 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,352 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,362 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,366 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,367 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,369 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,376 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,388 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,389 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:49,396 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,408 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,411 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,418 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,421 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:49,430 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,430 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,430 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,433 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,446 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,455 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,465 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,470 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,475 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,485 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,493 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,497 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,527 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,555 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,584 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,605 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,615 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,621 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,638 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,638 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,653 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,669 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,676 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,682 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,682 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,685 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,700 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,715 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,722 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,722 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,722 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,739 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,741 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,745 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,778 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,791 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,793 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,797 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:49,804 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,805 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,812 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,824 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,824 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,829 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,830 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:49,844 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,865 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,882 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,884 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,888 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,899 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,901 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:49,903 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,916 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,920 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,921 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:49,942 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:49,967 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:49,985 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,012 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,028 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,041 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,051 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,057 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,062 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,074 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,084 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,085 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,092 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,106 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,109 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,114 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,119 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,122 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,125 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,147 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,148 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,152 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,152 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,155 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,161 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,166 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,171 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,179 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,184 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,196 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,202 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,205 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,207 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,208 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,213 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,215 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,224 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,225 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,231 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,236 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,245 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,247 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,247 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,259 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:50,262 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,264 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,265 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,279 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,280 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,289 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,291 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:50,306 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,306 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,306 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,323 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,331 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,332 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,333 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,340 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,342 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,350 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,352 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,366 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,370 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,384 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,391 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,404 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,411 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,412 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,416 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:50,417 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,423 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,425 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,431 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,434 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,439 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,449 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:50,450 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,450 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,452 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,454 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,463 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,465 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,465 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,467 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,480 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,488 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,492 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,493 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,493 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,501 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,505 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,508 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,520 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,520 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,521 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,524 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,529 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,542 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,547 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,573 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,606 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,624 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,663 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,687 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,689 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,701 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,711 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,712 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,715 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,715 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,729 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,731 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,732 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,733 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,734 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,753 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,760 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,773 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,773 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,775 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,775 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,782 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,796 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,796 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,801 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,801 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,808 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,820 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,843 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,871 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,874 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,876 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,889 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,900 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,902 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,906 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,925 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,942 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,943 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,945 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,960 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:50,969 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:50,970 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,980 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:50,986 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:50,994 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:50,995 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,002 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,014 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:51,023 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,025 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,026 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,046 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,047 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,049 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,056 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,067 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,070 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,071 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,077 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,078 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,087 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,101 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,120 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,121 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,124 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,129 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,150 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,150 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,150 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,154 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,155 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,182 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,197 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,207 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,220 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,223 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,257 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,262 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,263 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,275 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:51,279 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,280 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,282 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,290 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,302 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,304 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:51,306 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,314 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,326 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,337 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,344 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,360 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,370 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,373 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,383 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,386 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,402 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,403 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,408 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,424 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,448 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,449 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,460 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,479 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,481 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,482 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,517 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,534 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,548 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,556 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,567 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,570 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,571 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,572 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,584 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,593 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,610 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,616 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,621 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,623 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,634 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,636 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,644 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,646 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,654 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,665 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,669 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,693 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,701 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,711 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,721 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,742 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,754 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,766 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,769 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,771 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,797 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,798 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,806 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,821 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,842 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,881 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,885 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,889 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,895 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,902 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,906 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,915 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,918 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,926 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,927 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,944 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,947 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,952 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,958 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:51,963 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,965 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,972 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:51,978 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:51,983 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:51,996 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,001 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,003 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,007 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,014 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,027 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,033 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,035 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,075 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,079 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,102 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,108 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,138 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:52,155 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,171 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:52,173 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,174 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,189 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,195 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,200 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,214 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,214 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,215 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,220 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,222 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,226 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,240 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,241 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,241 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,243 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,248 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,251 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,258 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,262 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,263 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,267 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,268 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,281 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,281 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,281 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,286 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,287 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,289 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,305 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,305 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,306 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,308 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,322 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,335 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,345 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,347 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,349 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,351 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,363 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:52,363 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,374 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,387 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,394 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:52,396 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,400 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,410 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,414 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,422 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,431 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,446 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,466 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,471 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,475 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,490 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,493 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,507 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,510 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,524 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,537 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,543 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,547 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,551 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,558 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,565 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,567 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,579 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,579 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,583 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,609 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,612 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,626 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,642 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,651 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,656 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,671 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,717 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,721 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,725 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,734 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,742 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,752 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,770 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,778 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,794 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,800 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,883 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,905 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,921 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,929 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,934 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,944 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,946 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,954 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,958 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,961 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,962 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:52,967 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:52,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,982 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:52,985 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,989 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:52,995 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,001 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,010 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,017 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,018 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,019 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,030 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,041 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,050 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,054 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,062 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,070 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,083 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,095 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,098 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,113 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,117 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,129 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,143 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,145 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,146 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,165 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,166 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,166 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,168 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,173 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,178 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,187 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,196 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,202 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,215 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,216 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,223 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,232 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,246 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,248 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,273 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,280 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,295 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,297 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,310 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,324 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:53,326 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,329 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,344 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,349 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,350 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,358 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:53,371 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,374 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,375 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,380 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,385 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,391 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,394 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,404 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,417 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,419 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,439 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,453 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,473 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,474 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,479 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,490 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,503 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,505 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,507 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,513 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,524 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,525 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,526 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,529 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,541 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,545 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,548 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,569 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,576 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,589 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,590 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,591 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,598 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,599 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,612 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,613 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,621 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,625 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,626 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,646 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,647 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,647 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,664 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,671 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,692 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,698 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,726 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,731 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,749 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,788 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,813 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,912 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,926 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:53,957 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:53,964 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,978 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:53,981 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:53,984 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,001 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,027 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,042 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,046 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,048 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,055 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,063 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,066 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,068 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,097 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,105 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,105 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,107 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,109 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,118 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,123 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,124 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,125 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,126 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,137 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,139 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,146 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,148 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,158 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,164 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,165 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,166 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,167 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,186 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,190 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,191 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,192 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,198 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,205 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,216 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,226 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,235 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,237 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,245 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,254 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,265 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,275 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,293 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,293 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,308 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,316 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,336 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,336 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,340 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,344 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,349 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:54,357 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,359 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,360 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,364 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,377 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,380 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:54,393 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,404 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,416 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,422 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,433 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,449 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,600 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,620 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,644 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,656 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,660 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,662 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,667 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,675 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,675 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,679 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,686 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,689 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,692 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,697 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,708 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,711 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,716 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,725 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,728 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,732 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,734 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,744 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,744 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,746 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,757 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,759 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,762 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,763 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,771 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,777 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,787 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,787 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,798 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,802 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,820 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,821 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,826 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,844 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,858 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,871 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,879 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,893 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,902 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,911 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,925 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,926 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,929 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,941 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,945 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,949 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,950 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:54,957 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,961 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:54,974 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:54,975 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,980 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,986 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:54,997 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,001 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,002 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,024 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,040 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,074 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,074 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,096 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,101 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,161 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,162 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,180 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,201 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,210 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,220 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,227 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,228 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,243 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,248 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,277 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,283 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,303 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,307 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,392 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,411 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:55,412 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,435 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,444 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:55,449 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,452 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,474 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,489 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,499 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,513 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,517 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,517 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,529 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,531 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,549 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,555 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,557 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,576 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,581 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,590 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,616 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,618 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,638 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,639 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,656 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,676 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,695 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,702 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,720 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,720 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,738 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,779 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,805 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:55,886 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:55,904 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:55,919 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:55,921 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:55,939 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:55,960 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:55,987 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:55,993 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,007 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,014 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,023 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,040 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,041 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,062 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,065 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,076 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,084 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,107 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,111 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,116 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,131 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,134 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,135 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,144 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,158 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,160 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,164 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,169 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,171 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,182 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,192 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,196 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,199 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,203 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,219 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,221 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,225 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,232 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,245 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,247 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,253 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,264 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,275 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,280 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,283 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,287 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,295 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,302 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,308 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,311 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,314 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,331 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,335 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,336 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,344 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,345 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,357 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,365 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,374 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,392 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,394 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,410 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,414 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,429 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,445 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,446 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:56,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,464 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,468 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,475 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:56,486 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,507 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,530 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,771 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,780 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,784 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,789 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,797 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,801 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,819 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,828 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,833 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,836 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,839 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,851 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,858 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,859 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,861 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,864 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,872 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,877 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,902 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:56,912 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:56,921 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:56,928 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:56,935 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,937 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:56,951 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:56,953 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,959 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:56,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:56,995 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,010 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,020 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,035 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,056 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:57,058 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,073 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,089 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:57,108 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,129 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,209 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,225 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,262 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,289 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,352 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,370 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,371 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,383 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,409 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,410 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,417 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,428 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,436 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,438 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,442 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,452 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,461 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,476 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,483 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,490 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,491 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,508 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,508 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,509 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,514 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,531 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,544 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,551 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,565 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:57,567 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,568 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,595 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,599 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:57,602 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,611 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,624 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,649 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,672 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,673 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,689 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,694 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,698 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:57,716 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,722 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:57,724 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,738 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,748 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,755 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,757 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,786 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,791 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,813 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,820 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,832 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,846 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,858 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,859 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,875 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,886 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,903 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,913 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,928 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,951 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:57,960 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:57,979 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:57,981 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:57,983 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:57,996 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:57,999 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,000 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,019 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,020 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,041 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,045 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,061 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:58,065 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,065 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,066 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,067 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,086 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:58,089 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,090 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,103 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,128 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,394 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,411 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,451 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,452 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,468 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,473 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,497 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:58,502 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,507 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,519 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,520 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:58,522 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,532 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,538 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,546 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:58,557 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,576 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:58,577 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,580 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,604 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,779 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,791 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,818 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,822 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:58,823 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,835 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,838 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,844 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:58,855 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,871 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,872 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,874 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,888 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:58,889 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,892 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,895 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:58,900 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:58,907 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,923 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,924 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:58,925 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:58,930 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,943 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,945 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:58,984 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,001 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,040 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,066 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,093 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,112 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,149 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,173 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,218 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,234 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,240 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:59,264 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:59,269 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,291 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,434 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,451 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,479 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,491 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,496 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,516 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,516 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:59,531 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,545 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:59,553 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,587 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,590 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,602 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,606 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,614 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:59,636 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:59,637 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,644 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,657 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,660 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,668 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,674 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,713 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,737 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,856 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:00:59,884 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:00:59,919 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,932 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:00:59,958 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:00:59,975 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:00:59,975 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:00:59,991 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:01:00,028 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:01:00,052 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:01:00,097 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:00,126 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:00,295 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:00,319 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:00,388 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:01:00,405 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:01:00,441 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:01:00,442 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:00,464 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:01:00,470 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:00,857 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:01:00,873 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:01:00,910 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:01:00,934 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:01:01,031 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:01,052 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:01,304 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:01,334 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:01,429 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:01,451 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:01,458 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 07:01:01,475 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 07:01:01,509 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 07:01:01,530 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 07:01:01,764 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:01,785 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:02,194 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:02,221 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:02,462 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:02,482 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:02,586 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:02,616 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:03,097 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:03,126 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:03,142 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:03,161 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:03,369 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:03,389 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:04,054 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:04,074 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:04,182 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:04,208 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:04,700 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:04,720 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:05,062 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:05,090 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 07:01:05,373 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 07:01:05,392 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 379, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 161, in forward
    queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 342, in forward
    attention_weights = attention_weights.softmax(-1)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 58.00 MiB (GPU 0; 191.98 GiB total capacity; 22.92 GiB already allocated; 0 bytes free; 23.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 264, in forward
    output = self.output_proj(output)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 40.00 MiB (GPU 0; 191.98 GiB total capacity; 22.41 GiB already allocated; 0 bytes free; 22.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 379, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 161, in forward
    queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 339, in forward
    attention_weights = self.attention_weights(query).view(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 58.00 MiB (GPU 0; 191.98 GiB total capacity; 14.62 GiB already allocated; 0 bytes free; 14.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 379, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 161, in forward
    queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 337, in forward
    sampling_offsets = self.sampling_offsets(query).view(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 114.00 MiB (GPU 0; 191.98 GiB total capacity; 12.31 GiB already allocated; 0 bytes free; 12.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 379, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py", line 143, in forward
    queries_rebatch = query.new_zeros(
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 58.00 MiB (GPU 0; 191.98 GiB total capacity; 12.07 GiB already allocated; 0 bytes free; 12.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 340, in get_bevs
    prev_bev = self.get_history_bev(prev_img, prev_img_metas)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 325, in get_history_bev
    img_feats_list = self.extract_img_feat(img=imgs_queue, len_queue=len_queue)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 162, in extract_img_feat
    img_feats = self.img_backbone(img)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmdetection/mmdet/models/backbones/resnet.py", line 636, in forward
    x = self.conv1(x)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 2.12 GiB (GPU 0; 191.98 GiB total capacity; 34.00 GiB already allocated; 0 bytes free; 34.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 244, in forward
    output = MultiScaleDeformableAttnFunction.apply(
  File "/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py", line 105, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/multi_scale_deformable_attn_function.py", line 118, in forward
    output = ext_module.ms_deform_attn_forward(
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 32.73 GiB already allocated; 0 bytes free; 33.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 163, in forward_train
    losses_track, outs_track = self.forward_track_train(img, gt_bboxes_3d, gt_labels_3d, gt_past_traj, gt_past_traj_mask, gt_inds, gt_sdc_bbox, gt_sdc_label,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 555, in forward_track_train
    frame_res = self._forward_single_frame_train(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 385, in _forward_single_frame_train
    bev_embed, bev_pos = self.get_bevs(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py", line 348, in get_bevs
    bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py", line 149, in get_bev_features
    bev_embed = self.transformer.get_bev_features(
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/transformer.py", line 179, in get_bev_features
    bev_embed = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 211, in forward
    output = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/encoder.py", line 356, in forward
    query = self.attentions[attn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py", line 194, in forward
    query = torch.cat([value[:bs], query], -1)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 32.55 GiB already allocated; 0 bytes free; 33.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31185 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 31183) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 766, in <module>
    main()
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 31184)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 31186)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 31187)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 31188)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 31189)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 31190)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-22_07:01:42
  host      : hjbog-srdc-20.amd.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31183)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
